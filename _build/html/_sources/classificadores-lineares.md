<style>
    main {
        text-align: justify;
    }
    legend {
        font-size: 16px;
    }
</style>

# CapÃ­tulo 2: Classificadores lineares


Os classificadores lineares compartilham o conceito de separaÃ§Ã£o entre classes atravÃ©s de funÃ§Ãµes discriminantes expressas como combinaÃ§Ã£o linear entre coeficientes (pesos) e as caracterÃ­sticas dos padrÃµes. A determinaÃ§Ã£o desses coeficientes segundo cada mÃ©todo encontrado na literatura segue um processo particularm porÃ©m com o objetivo final de alcanÃ§ar a melhor configuraÃ§Ã£o possÃ­vel. Cabe destacar que parte dos mÃ©todos lineares abordados aqui abrem caminho para a formalizaÃ§Ã£o de determinados mÃ©todos nÃ£o lineares.

## Classificadores e problemas lineares

Da forma mais simplista possÃ­vel, podemos definir que um classificador Ã© linear desde que o processo de discriminaÃ§Ã£o das classes abrangidas pelo problema seja conduzida atravÃ©s de superfÃ­cies de decisÃ£o lineares. Uma superfÃ­cie de decisÃ£o linear consiste no lugar geomÃ©trico que torna nula uma funÃ§Ã£o discriminante linear. Geralemente, uma funÃ§Ã£o linear Ã© dada por:

<div align=center>
    
$\begin{equation}
g(\textbf{x})=\textbf{w}^{T}\textbf{x} + ğœ”_{0} \tag{2.1}
\end{equation}$ </div>

em que $\textbf{x}$, $\textbf{w} \in \chi âŠ† \mathbb{R}^{n} $ e $ğœ”_{0} \in \mathbb{R}$. Cabe observar que a expansÃ£o de $\textbf{w}^{T}\textbf{x} + ğœ”_{0}$ gera uma combinaÃ§Ã£o linear, e ainda, $\textbf{w}^{T}\textbf{x}$ corresponde ao produto interno entre $\textbf{w}$ e $\textbf{x}^{1}$.

A fim de exibir determinadas caracterÃ­sticas e relaÃ§Ãµes relevantes a respeito das superfÃ­cies de decisÃ£o geradas por $g(\textbf{x})$, vamos considerar $\mathbb{R}^{2}$ como espaÃ§o de atributos. Por sua vez, com o intuito de reforÃ§ar o entendimento, a Figura 2.1 apoia esta discussÃ£o.

<div align="center">

![figura21](images/lineares_figura22.png "figura 2.1") <legend>Figura 2.1 - CaracterÃ­sticas de uma superfÃ­cie de decisÃ£o linear.
</legend> </div>

Como mencionado, a superfÃ­cie de decisÃ£o corresponde a um subconjunto de vetores $\textbf{x}$ no espaÃ§o de atributos que torna $g(\textbf{x})=0$. Ao tomar $\textbf{x}_{1}$ e $\textbf{x}_{2}$ sobre a superfÃ­cie de decisÃ£o, verificamos que:

<div align=center>
    
$\begin{equation}
\textbf{w}^{T}\textbf{x} + ğœ”_{0} = \textbf{w}^{T}\textbf{x}_{2} + ğœ”_{0} \Leftrightarrow \textbf{w}^{T}(\textbf{x}_{1}-\textbf{x}_{2})=0
\end{equation}$ </div>

levando a concluir que $\textbf{w}$ Ã© ortogonal Ã  superfÃ­cie de decisÃ£o linear, pois $\textbf{x}_{1}-\textbf{x}_{2}$ pode ser admitindo como um vetor que determina a superfÃ­cie de decisÃ£o (neste caso, uma reta), e a ortogonalidade mencionada decorre do produto interno nulo entre $\textbf{w}$ e o vetor $\textbf{x}_{1}-\textbf{x}_{2}$.

Considerando agora outros dois vetores $\textbf{x}_{3}=(x_{31},0)$ e $\textbf{x}_{4}=(0,x_{42})$ que tambÃ©m ocupam a superfÃ­cie de decisÃ£o, salvo detalhe que $\textbf{x}_{3}$ e $\textbf{x}_{4}$ interceptam o primeiro e segundo eixo do espaÃ§o de atributos, respectivamente. Com isso:

<div align=center>
    
$\begin{equation}
g(\textbf{x}_{3})=\textbf{w}^{T}\textbf{x}_{3} + ğœ”_{0} = 0 \Rightarrow ğœ”_{1}x_{31} + ğœ”_{2}0 + ğœ”_{0} = x_{31} = -\frac{ğœ”_{0}}{ğœ”_{1}}
\end{equation}$ </div>

<div align=center>

$\begin{equation}
g(\textbf{x}_{3})=\textbf{w}^{T}\textbf{x}_{4} + ğœ”_{0} = 0 \Rightarrow ğœ”_{1}0 + ğœ”_{2}x_{42} + ğœ”_{0} = x_{42} = -\frac{ğœ”_{0}}{ğœ”_{2}}
\end{equation}$ </div>

Esse resultado nos permite concluir que a distÃ¢ncia entre a superfÃ­cie de decisÃ£o e a origem do espaÃ§o de atributos equivale a $\frac{|ğœ”_{0}|}{||\textbf{w}||}$. Ainda, com base no mesmo conceito de distÃ¢ncia entre ponto e reta, podemos concluir que o mÃ³dulo de $g(\textbf{x})$ expressa a distÃ¢ncia entre $\textbf{x}$ e a superfÃ­cie de decisÃ£o.

AlÃ©m da noÃ§Ã£o de distÃ¢ncia entre os padrÃµes/vetores e a superfÃ­cie de decisÃ£o, Ã© de extrema importÃ¢ncia o valor do retorno gerado pela funÃ§Ã£o discriminante. Ao tomar um $\textbf{x}$ qualquer, tal que $g(\textbf{x}) > 0$, podemos concluir que tal vetor estÃ¡ afastado da superfÃ­cie $g(\textbf{x})=0$ no mesmo sentido do vetor $\textbf{w}$. De forme similar, $g(\textbf{x}) < 0$ indica que o vetor ortogonal Ã  superfÃ­cie de decisÃ£o com extremidade em $\textbf{x}$ possui sentido oposto a $\textbf{w}$. Esta observaÃ§Ã£o a respeito do sinal de $g(\textbf{x})$ caracteriza a regra de decisÃ£o de um classificador linear, usualmente expressa por:

<div align=center>

$\begin{equation}
g(\textbf{x})=\textbf{w}^{T}\textbf{x} + ğœ”_{0}  \left \{ \begin{matrix} >0 â‡’ \textbf{x} \in ğœ”_{1} \\ < 0 â‡’ \textbf{x} \in ğœ”_{2} \end{matrix} \right. \tag{2.2}
\end{equation}$ </div>

De acordo com a equaÃ§Ã£o acima, verifica-se que um classificador linear possibilita a distinÃ§Ã£o entre duas classes somente. A aplicabilidade desta abordagem em problemas de classificaÃ§Ã£o que abrangem mais de duas classes exige o uso de estratÃ©gias, as quais serÃ£o introduzidas mais adiante.

Nesta breve introduÃ§Ã£o sobre classificadores lineares, podemos notar a grande diferenÃ§a conceitual existente entre esta abordagem e as regras de classifiaÃ§Ã£o baseadas na Teoria da DecisÃ£o de Bayes. A simplicidade conceitual e maior facilidade no tratamento computacional tornam atrativos os classificadores lineares.

Antes de prosseguir os estudos sobre os classificadores lineares, Ã© importante destacarmos a diferenÃ§a entre classificadores lineares e problemas linearmente separÃ¡veis. Conforme jÃ¡ definido, os classificadores lineares sÃ£o estruturados segundo funÃ§Ãµes discriminantes com forma equivalente Ã  apresentada na EquaÃ§Ã£o 2.1. Por outro lado, um problema de classificaÃ§Ã£o Ã© linearmente separÃ¡vel desde que seja possÃ­vel obter, ao menos, uma superfÃ­cie de decisÃ£o linear que distinguia dois tipos de objetos cujas classes sejam conhecidas de antemÃ£o (Figura 2.2). Isso nos permite concluir que a capacidade de separaÃ§Ã£o atravÃ©s de uma superfÃ­cie linear Ã© uma propriedade dos dados a serem classificados e nÃ£o do classificador em si.

<div align="center"> 

![figura21](images/lineares_figura21.png "figura 2.2") <legend>Figura 2.2 - Exemplos de casos de separabilidade. Para dados linearmente separÃ¡veis, Ã© garantida a existÃªncia de uma superfÃ­cie linear capaz de particionar o espaÃ§o de atributos em dois subconjuntos de classes distintas. O mesmo nÃ£o Ã© garantido para o caso nÃ£o linearmente separÃ¡vel.
</legend> </div>
