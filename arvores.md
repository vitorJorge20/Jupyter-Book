<style>
    legend {
        font-size: 16px;
    }
    main {
        text-align: justify;
    }
</style>

# 3.2 √Årvores de decis√£o

No contexto da teoria de garfos, uma √°rvore bin√°ria √© estruturada atrav√©s de v√©rtices (ou n√≥s) e rela√ß√£o hier√°rquicas. Partindo de um n√≥, usualmente denominando raiz, podem ser definidas ramifica√ß√µes bipartidas sucessivas, as quais determinam novos n√≥s. Um n√≥ √© dito interno desde que produza ramifica√ß√µes, caso contr√°rio √© denominado folha. A Figura 3.4 ilustra a estrutura discutida.

<div align="center"> 

![figura34](images/figura34.png "figura 3.4") <legend>Figura 3.4 - Exemplo de √°rvore de decis√£o bin√°ria.</legend> </div>

A estrutura fornecida por este tipo de garfo favoreceu uma abordagem de classifica√ß√£o n√£o linear distinta das discuss√µes anteriores, denominadas √°rvores de decis√£o. Em geral, a modelagem de uma √°rvore de decis√£o √© baseada em verifica√ß√µes sucessivas sobre os atributos dos padr√µes em um conjunto de treinamento. Tais verifica√ß√µes consistem em determinar um limiar que atuar√° apenas sobre um dos atributos  dos padr√µes e, por sua vez, que permite dividir os exemplos em dois subconjuntos cujas respectivas variabilidades s√£o minimizadas. Esse processo √© realizado sucessivamente at√© que subconjuntos gerados apresentem variabilidade inferior a um valor preestabelecido. A Figura 3.5 traz um exemplo de √°rvore de decis√£o e a separa√ß√£o proporcionada sobre os dados no espa√ßo de atributo.

<div align="center"> 

![figura35](images/figura35.png "figura 3.5") <legend>Figura 3.5 - Particionamento efetuado sobre o espa√ßo de atributos pela √°rvore de decis√£o.</legend> </div>

Entre diferentes propostas existentes na literatura para constru√ß√£o de √°rvores de decis√£o aplicadas √† classifica√ß√£o de padr√µes e regress√£o, as discuss√µes que seguem referem-se ao m√©todo denominado CART ($\textit{Classification and Regression Trees}$).

Formalmente, partimos de $D = \{(\textbf{x}_{i},y_{i}) \in  \mathcal{X} √ó \mathcal{Y}: \ i=1,...m\}$ como conjunto de treinamento, sendo $\mathcal{X}‚äÜ\mathbb{R}^{n}$ e $\mathcal{Y}=\{1,...,c\}$, o qual √© relacionado ao conjunto de classes $\Omega=\{\omega_{1},...\omega_{c}\}$. Como j√° empregado, $y_{i}$ atua como indicador de classe, talque $y_{i}=j$ implica que que $\textbf{x}_{i}$ est√° associado √† classe $\omega_{j}$.

Conforme evidenciado, ao longo do processo de constru√ß√£o da √°rvore de decis√£o, s√£o utilizados subconjuntos $D$ cada vez mais restritos. A fim de generalizar as nota√ß√µes, ser√° denominado $Q$ o subconjunto de elementos de $D$ considerado em um n√≥ desta √°rvore. Em um primeiro momento, na raiz da √°rvore, o conjunto $Q$ equivale a $D$.

Assim, para um n√≥ qualquer, $Q$ √© composto por $q$ pares $(\textbf{x},y)$. Independentemente do comportamento dos padr√µes $\textbf{x}$ em $Q$ e do atributo considerado, existem, no m√°ximo, $ùúè\leq q$ valores que permitem a divis√£o deste conjunto em duas partes. No entanto, devido a poss√≠veis repeti√ß√µes de valores que cada um dos atributos pode apresentar, a quantidade de candidatos a limiar para divis√£o de $Q$ pode variar em fun√ß√£o dos atributos.

Seja $Q_{k} = \{x_{k}:(\textbf{x},y)\in Q; \ \textbf{x}=[x_{1},...,x_{k},...,x_{n}]\}$ o conjunto de valore observados, sem separa√ß√£o, sobre o k-√©simo atributo dos exemplos de Q. No contexto desta discuss√£o, podem ser definidos $ùúè_{kh}$ valores, com $k=2,...n$ e $h=1,...,\#Q_{k}$, que, por sua vez, determinam $Q_{inf}(ùúè_{kh})$ e $Q_{sup}(ùúè_{kh})$ como subconjuntos de $Q$, cujo valor do k-√©simo atributo de seus vetores $\textbf{x}$ √© inferior ou maior-ou-igual a $ùúè_{kh}$, respectivamente. Ou seja:

<div align="center"> 

$\begin{equation}
Q_{inf}(ùúè_{kh}) = \{(\textbf{x},y) \in Q:x_{k} < ùúè_{kh}\}
\end{equation}$

$\begin{equation}
Q_{sup}(ùúè_{kh}) = \{(\textbf{x},y) \in Q:x_{k} \geq ùúè_{kh}\}
\end{equation}$ </div>

A fim de quantificar a variabilidade de classes em $Q$, tamb√©m denominada impureza, √© usual o emprego da medida de Entropia da Informa√ß√£o, expressa na Equa√ß√£o 3.7. Cabe observar que o valor m√°ximo desta medida √© atingido quando as probabilidades de ocorr√™ncia das classes se tornam iguais. Logo, sua minimiza√ß√£o leva √† m√≠nima incerteza com que os elementos deste conjunto s√£o associados a uma das classes do problema.

<div align="center"> 

$\begin{equation}
I(Q) = - \sum_{j=1}^{c} P(\omega_{j}|Q)\log_{2}P(\omega_{j}|Q) \tag{3.7}
\end{equation}$

em que:

$\begin{equation}
P(\omega_{j}|Q)=\frac{1}{\#Q} \sum_{i=1}^{\#Q} \delta_{j}(y_{i})
\end{equation}$

$\begin{equation}
\delta_{j}(y_{i}) = \left \{ \begin{matrix} 1; \ se \ y_{i}=j \\ 0; \ caso \ contr√°rio \end{matrix} \right.
\end{equation}$ </div>

Dessa forma, diante da escolha de um limiar $ùúè_{kh}$, √© poss√≠vel medir a redu√ß√£o da impureza que ser√° proporcionada ao subdividir $Q$ em $Q_{inf}(ùúè_{kh})$ e $Q_{sup}(ùúè_{kh})$, conforme definido na Equa√ß√£o 3.7. √© importante destacar que a determina√ß√£o de $ùúè_{kh}$ √© alcan√ßada de forma exaustiva com base nos valores de $Q_{k}$, para $k=1,...,n$.

<div align="center"> 

$\begin{equation}
\Delta I(Q;ùúè_{kh})= I(Q)-\frac{\#Q_{inf}(ùúè_{kh})}{\#Q}I(Q_{inf}(ùúè_{kh}))-\frac{\#Q_{sup}(ùúè_{kh})}{\#Q}I(Q_{sup}(ùúè_{kh})) \tag{3.8}
\end{equation}$ </div>

Nestas condi√ß√µes, quando $\Delta I(Q;ùúè_{kh})$ supera um limiar $\zeta \in \mathbb{R}_{+}$ preestabelecido e a quantidade de elementos $Q$ tamb√©m supera um n√∫mero m√≠nimo $\psi \in \mathbb{N}^{*}$, torna-se justificada a necessidade de divis√£o entre $Q_{inf}(ùúè_{kh})$ por $D_{sup}(ùúè_{kh})$. Nesse caso, todos os processos discutidos anteriormente s√£o conduzidos em car√°ter recursivo sobre cada subconjunto obtido, os quais caracterizam novos n√≥s da √°rvore de decis√£o em constru√ß√£o e possuem $Q$ igual a $Q_{inf}(ùúè_{kh})$ e $D_{sup}(ùúè_{kh})$, respectivamente.

Caso a divis√£o n√£o se justifique, o n√≥ associado ao conjunto (local) $Q$ √© caracterizado como uma "folha" e, por sua vez, deve representar uma das classes $\omega^{*} \in \Omega$. Para tal associa√ß√£o, a seguinte regra pode ser empregada:

<div align="center"> 

$\begin{equation}
\omega^{*} = arg\max_{\omega_{j}\in\Omega}P(\omega_{j}|Q) \tag{3.9}
\end{equation}$ </div>

Uma vez cessado o crescimento da √°rvore de decis√£o, o processo de treinamento √© finalizado. Por conseguinte, o processo de classifica√ß√£o de um padr√£o n√£o rotulado √© efetuado a partir de sua apresenta√ß√£o na raiz da √°rvore de decis√£o, sendo, ent√£o, submetido sequencialmente √†s diversas regras relacionadas aos n√≥s at√© que uma folha seja alcan√ßada. A classe representada pela folha em quest√£o determina a classifica√ß√£o do padr√£o em estudo.

Como exemplo de aplica√ß√£o, a Figura 3.6 ilustra o comportamento das superf√≠cies  de decis√£o delineadas pelo m√©todo CART sobre a conhecida base de dados Iris, que apresenta um problema multiclasse que possui caracter√≠sticas lineares e n√£o lineares. O particionamento do espa√ßo de atributos atrav√©s de limiares constantes em rela√ß√£o aos atributos, similar √† concep√ß√£o da Figura 3.6, proporciona o aspecto de "setores".

<div align="center"> 

![figura36](images/figura36.png "figura 3.6") <legend>Figura 3.6 - Exemplo de aplica√ß√£o do m√©todo CART utilizando a base de dados Iris.</legend> </div>

A partir da biblioteca Scikit-Learn, com a importa√ß√£o do m√≥dulo $\textbf{tree}$, a implementa√ß√£o do classificador CART torna-se acess√≠vel via $\textbf{DecisionTreeClassifier}$. O c√≥digo 3.2 apresenta apenas o trecho de importa√ß√£o do m√≥dulo citado e a instancia√ß√£o de um classificador CART. O uso das fun√ß√µes de treinamento e predi√ß√µes/classifica√ß√£o √© id√™ntico ao caso do classificador SVM. Os par√¢metros $\textbf{criterion}$,$\textbf{min_samples_split}$ e $\textbf{min_impurity_decrease}$ referem-se ao tipo da medida de impureza e aos par√¢metros $\psi$ e $\zeta$, respectivamente. A configura√ß√£o $\textbf{criterion='entropy'}$ determina que a constru√ß√£o da √°rvore de decis√£o √© baseada na medida de entropia.

```
#C√≥digo 3.2 - Instancia√ß√£o do Classificador √Årvore de Decis√£o

#importa√ß√£o
from sklearn import tree

#Instancia√ß√£o do classificador 'g'
g = DecisionTreeClassifier(criterion='entropy',min_samples_split=2,min_impurity_decrease=10**(-7))
```

```
#Exemplo 2 - Aplica√ß√£o do classificador de √°rvore de decis√£o ao dataset Iris e plotagem das superf√≠cies de decis√£o

from sklearn.datasets import load_iris

iris = load_iris()

import matplotlib.pyplot as plt
import numpy as np

from sklearn.datasets import load_iris
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.tree import DecisionTreeClassifier

# Par√¢metros
n_classes = 3
plot_colors = "ryb"
plot_step = 0.02


for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):
    # Selecionando os atributos aos pares
    X = iris.data[:, pair]
    y = iris.target

    # Treino
    clf = DecisionTreeClassifier(criterion='entropy',min_samples_split=2,min_impurity_decrease=10**(-7)).fit(X, y)

    # Plotagem da superf√≠cie de decis√£o
    ax = plt.subplot(2, 3, pairidx + 1)
    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)
    DecisionBoundaryDisplay.from_estimator(
        clf,
        X,
        cmap=plt.cm.RdYlBu,
        response_method="predict",
        ax=ax,
        xlabel=iris.feature_names[pair[0]],
        ylabel=iris.feature_names[pair[1]],
    )

    # Plotagem dos pontos de treinamento
    for i, color in zip(range(n_classes), plot_colors):
        idx = np.where(y == i)
        plt.scatter(
            X[idx, 0],
            X[idx, 1],
            c=color,
            label=iris.target_names[i],
            cmap=plt.cm.RdYlBu,
            edgecolor="black",
            s=15,
        )

plt.suptitle("Superf√≠cie de decis√£o de √°rvores de decis√£o treinadas em pares de atributos")
plt.legend(loc="lower right", borderpad=0, handletextpad=0)
_ = plt.axis("tight")
```