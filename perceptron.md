<style>
    legend {
        font-size: 16px;
    }
    main {
        text-align: justify;
    }
</style>

# 2.2 Perceptron

A representa√ß√£o gen√©rica do Perceptron, inspirada no modelo de neur√¥nio proposto por Warren McCulloch e Walter Pitts [McCulloch and Pitts, 1943], √© ilustrada na Figura 2.3. Segundo esta representa√ß√£o, podemos observar cico elementos b√°sicos: (i) √† esquerda, uma camada composta por $n$ receptores, respons√°veis por receber as informa√ß√µes sin√°pticas $x_{1}...,x_{n}$; (ii) um conjunto de conex√µes sin√°pticas, que pondera as informa√ß√µes de entrada segundo os pesos $w_{1}...,w_{n}$; (iii) uma entrada constante e igual a +1, ponderada por $w_{0}$; (iv) no n√∫cleo do neur√¥nio, um concentrador (i.e., uma soma $‚àë$) dis sinais de entrada que induz um campo sin√°ptico e repassa para; (v) a fun√ß√£o de ativa√ß√£o $g$, a qual √© respons√°vel por gerar uma resposta.

<div align="center">

![figura1](images/perceptron23.png "figura 25") <legend>Figura 2.3 - O modelo Perceptron.</legend> </div>


Com base nos elementos do modelo introduzidos, √© formalizado um m√©todo capaz de realizar classifica√ß√£o de realizar classifica√ß√£o de padr√µes. Para tal, considere um problema linearmente separ√°vel que envolve duas classes e no qual √© conhecido um conjunto de observa√ß√µes $D = \{(\textbf{x}_{i},y_{i})\in \textit{X} \times \textit{Y}:i=1,...,m\}$. Segundo esta nota√ß√£o, temos que $\textbf{x}_{i}$ est√° associado exclusimamente √† $ùúî_{1}$ ou $ùúî_{2}$ quando $y_{i}$ equivale a +1 ou -1, respectivamente.

Para fins de simplica√ß√£o das discuss√µes, consideremos ainda que o problema de classifica√ß√£o ser√° conduzido sobre um espa√ßo de atributos estendido, isto √©, $\textbf{x} = [1,x_{1},...,x_{n}]^{T} \in œá \subseteq \mathbb{R}^{n+1}$, fazendo com que as fun√ß√µes discriminantes sejam reduzidas √† forma:

<div align="center">

$\begin{equation}
g(\textbf{x}) = \textbf{w}^{T} \textbf{x} \tag{2.3}
\end{equation}$ 

com $\textbf{w}=[ùúî_{0},ùúî_{1},...,ùúî_{n}]^{T}$. </div>

Nessas condi√ß√µes, o m√©todo Perceptron busca pelo $\textbf{w}$ que proporciona $g(\textbf{x}_{i})>0$, se $y_{i} = +1$, e $g(\textbf{x}_{i})<0$ se $y_{i} = -1$, para $i=1,...,m$. Uma solu√ß√£o para este problema pode ser alcan√ßanda atrav√©s da minimiza√ß√£o da seguinte fun√ß√£o:

<div align="center">

$\begin{equation}
J(\textbf{w}) = \sum_{i=1}^{m} (Œ¥(\textbf{x}_{i},\textbf{w})\textbf{w}^{T}\textbf{x}_{i}); Œ¥(\textbf{x}_{i};\textbf{w}) = \left \{ \begin{matrix} -1, \ se \  y_{i} = +1 \ e \ \textbf{w}^{T} \textbf{x}_{i} < 0 \\ +1, \ se \ y_{i} = -1 \ e \ \textbf{w}^{T} \textbf{x}_{i} > 0 \\ 0, \ caso \ contr√°rio \ \end{matrix}\right. \tag{2.4}
\end{equation}$ </div>

Vale notar que $Œ¥(\textbf{x};\textbf{w})$ atua como "fun√ß√£o penalizadora" no processo de treinamento do Perceptron. A aplica√ß√£o da pena ocorre de acordo com o par√¢metro atual \textbf{w}. Consequentemente, teremos $J(\textbf{w})=0$ quando obtivemos $\textbf{w}$ tal que $Œ¥(\textbf{x}_{i},\textbf{w})=0$ para $i=1,...,m$, o que implica nenhuma penaliza√ß√£o por erro de classifica√ß√£o durante o processo de treinamento.

A minimiza√ß√£o de $J(\textbf{w})$ pode ser realizada com uso do algoritmo Gradiente Descendente. Atrav√©s desta estrat√©gia iterativa, s√£o obtidas atualiza√ß√µes sucessivas para $\textbf{w}$. Denotando por $\textbf{w}^{(k)}$ a k-√©sima aproxima√ß√£o obtida para $\textbf{w}$, temos:

<div align="center">

$\begin{equation}
\textbf{w}^{(k+1)}=\textbf{w}^{(k)}-Œ∑_{k}\frac{\partial J(\textbf{w})}{\partial\textbf{w}}\left|\begin{matrix} _{\textbf{w}=\textbf{w}^{(k)}}\end{matrix}\right. \tag{2.5}
\end{equation}$ </div>

em que $Œ∑_{k} \in\mathbb{R}_{+}$ √© um fator de corre√ß√£o que controla a atualiza√ß√£o de $\textbf{w}^{(k)}$. Com rela√ß√£o a $\textbf{w}^{(0)}$, os valores das componentes podem ser inicializados de forma aleat√≥ria ou mesmo nula.

Uma vez que $\frac{\partial J(\textbf{w})}{\partial\textbf{w}} = \frac{\partial (\sum_{i=1}^{m}(Œ¥(\textbf{x}_{i};\textbf{w})\textbf{w}^{T}\textbf{x}_{i}))}{\partial\textbf{w}} = \sum_{i=1}^{m}(Œ¥(\textbf{x}_{i};\textbf{w})\textbf{x}_{i} $, a Equa√ß√£o 3 √© reescrita por:

<div align="center">

$\begin{equation}
\textbf{w}^{(k+1)}=\textbf{w}^{(k)}-Œ∑_{k}\sum_{i=1}^{m}(Œ¥(\textbf{x}_{i};\textbf{w})\textbf{x}_{i} \tag{2.6}
\end{equation}$ </div>

Ap√≥s um n√∫mero finito de itera√ß√µes, o algortimo Perceptron deve convergir. Cabe ressaltar que a converg√™ncia ocorrer√° desde que a sequ√™ncia de fatores $\{ Œ∑_{k}\}_{k=0}^{‚àû}$ seja tal que $\lim_{k‚Üí‚àû}\sum_{j=0}^{k}Œ∑_{j}^{2} < ‚àû$ e $\lim_{k‚Üí‚àû}\sum_{j=0}^{k}Œ∑_{j} ‚Üí ‚àû$. Uma escolha usual para este fator, e que respeita as condi√ß√µes apontadas, √© $Œ∑_{k}=\frac{z}{k+1}$, com $z>1$. Por fim, uma vez alcan√ßada a converg√™ncia, o processo de classifica√ß√£o √© dado de modo similar √† regra estabelecida na equa√ß√£o 2.2.

Como consequ√™ncia do comportamento de $Œ¥(\cdot;\cdot)$, a fun√ß√£o objetivo que caracteriza o treinamento do Perceptron torna-se linear por partes. Este comportamento possibilita que diferentes configura√ß√µes de valores (i.e., pares da regi√£o central) de pesos sin√°pticos proporcionem a minimiza√ß√£o de fun√ß√£o objetivo.

A Figura 2.4 apresenta a aplica√ß√£o do algoritmo Perceptron sobre um conjunto de dados linearmente separ√°veis. As superf√≠cies de decis√£o obtidas ao longo das itera√ß√µes s√£o identificadas pelas retas em tons claros no gr√°fico da esquerda, para as quais √© observado um movimento de converg√™ncia em dire√ß√£o a uma solu√ß√£o. O resultado final de separa√ß√£o √© apresentado no gr√°fico da direita.

<div align="center">

![figura1](images/perceptron24.png "figura 26") <legend>Figura 2.4 - Processo de ajuste da superf√≠cie de decis√£o pelo algoritmo Perceptron (esquerda) e separa√ß√£o final dos dados (direita).
</legend> </div>

Uma implementa√ß√£o do m√©todo Perceptron, segundo os elementos introduzidos nesta se√ß√£o, √© dada pela fun√ß√£o $\mathbb{perceptron}$ apresentada no C√≥digo 2.1. Nesta implementa√ß√£o, s√£o verificadas outras duas fun√ß√µes $\mathbb{eta\_update}$ e $\mathbb{delta\_check}$, respons√°veis pela atualiza√ß√£o da taxa de aprendizado e pelo c√°lculo de $Œ¥(\cdot;\textbf{w})$. Ainda, uma vez obtido $\textbf{w}$, a predi√ß√£o do indicador de classe (i.e., +1 ou -1) pode ser efetuada com uso da fun√ß√£o ($\mathbb{predictor\_perceptron}$). Tais fun√ß√µes est√£o contidas no C√≥digo 2.2.

```
#Importa√ß√µes
import random
import numpy as np
```

```
#C√≥digo 2.1

def perceptron(x,y):
  dim = x.shape[1] #Dimens√£o do espa√ßo de atributos
  k = 0 #Contador de itera√ß√£o
  z = 1 #Usado no ajuste da taxa de aprendizado

  #Inicializa aleatoriamente w no espa√ßo estendido
  w = np.random.normal(0,1,dim+1) #Distribui√ß√£o N(0,1)

#Inicializa√ß√£o da taxa de aprendizado
  eta = eta_update(z,k)

  while True:
    S = np.zeros(3) #Contabiliza desvio dos casos errados
    count = 0       #Conta casos errados
    for xi, yi in zip(x,y):
      if delta_check(xi,yi,w) * w.dot(np.hstack((1,xi))) > 0:
        S += delta_check(xi,yi,w)*np.hstack((1,xi))
      if delta_check(xi,yi,w) != 0: count += 1

    #Atualiza√ß√£o das vari√°veis

    w0 = np.copy(w) #Faz uma c√≥pia de w para compara√ß√£o
    w = w - eta * S #Atualiza w

    #Reinicia a taxa de aprendizado caso n√£o convirja
    #For√ßa um retorno de "eta" para 1

    if np.linalg.norm(w0-w) < 10**-4: z = k+1

    eta = eta_update(z,k) #Atualiza√ß√£o do "eta"

    k += 1

    if (count==0): break #Obteve a separa√ß√£o linear!
  return w #O retorno consiste no valor de pesos w
```

```
#C√≥digo 2.2

#Express√£o usada na utiliza√ß√£o da taxa de aprendizado

def eta_update(z,k):
  return z/(k+1)

#Verifica√ß√£o de erro/acerto e respectiva penaiza√ß√£o

def delta_check(x,y,w):
  if (y>0) and (w.dot(np.hstack((1,x))) < 0): return -1
  if (y<0) and (w.dot(np.hstack((1,x))) > 0): return +1
  return 0
#Fun√ß√£o de predi√ß√£o da classe de x segundo o vetor w
def perceptron_predict(x,w):
  if (w.dot(np.hstack((1,x)))>0): return 1
  else: return -1
```

## Perceptron sequencial

  Segundo as discuss√µes na se√ß√£o anterior, o algoritmo Perceptron realiza a busca por $\textbf{w}$ de forma iterativa. Diante da Equa√ß√£o 3.6, podemos verificar que a corre√ß√£o de $\textbf{w}$ √© guiada por $\sum_{i=1}^{m}Œ¥(\textbf{x}_{i};\textbf{w})\textbf{x}_{i}$ ap√≥s a aplica√ß√£o da fun√ß√£o $Œ¥(‚ãÖ;‚ãÖ)$ sobre todos os $\textbf{x}_{i}$ em $D$. Como variante desta formula√ß√£o original, o processo de atualiza√ß√£o pode ser conduzido ao passo que cada observa√ß√£o √© apresentada, ou seja:

  <div align="center">

  $\begin{equation}
  \textbf{w}^{(k+1)} = \textbf{w}^{(k)} - Œ∑_{k}Œ¥(\textbf{x}_{i};\textbf{w}^{(k)}) \tag{2.7}
  \end{equation}$ </div>

Nesta proposta, a atualiza√ß√£o de $\textbf{w}^{(k+1)}$ ocorre somente quando a fun√ß√£o $Œ¥(\textbf{x}_{i};\textbf{w}^{(k)})$, diante da parametriza√ß√£o atual representada por $\textbf{w}^{(k)}$, identifica um erro de classifica√ß√£o, j√° que a parcela $Œ∑_{k}Œ¥(\textbf{x}_{i};\textbf{w}^{(k)})$ se torna nula diante das classes apresentadas sucessivamente atrav√©s de ciclos. Com rela√ß√£o √† inicializa√ß√£o $\textbf{w}^{(0)}$ e atualiza√ß√£o de $Œ∑_{k}$, podem ser adotados os mesmos crit√©rios antes discutidos.

Analogamente √† se√ß√£o anterior, a Figura 2.5 apresenta o processo de obten√ß√£o da superf√≠cie de decis√£o atrav√©s do algoritmo Perceptron, por√©m atualizado de forma sequencial. Em compara√ß√£o com a vers√£o anterior, nota-se um menor n√∫mero de itera√ß√µes exigidas, pois h√° uma menor concentra√ß√£o de superf√≠cies testadas (representadas pelas linhas claras e finas). No entanto, √© v√°lido ressaltar que este √© um comportamento que pode estar relacionado √† disposi√ß√£o dos padr√µes.

<div align="center">

![figura1](images/perceptron25.png "figura 26") <legend>Figura 2.5 - Processo de ajuste da  superf√≠cie de decis√£o pelo algoritmo Perceptron, baseado em ajustes sequenciais (esquerda) e separa√ß√£o final dos dados¬†(direita).</legend> </div>

## Aplica√ß√£o sobre dados n√£o linearmente separ√°veis

Apesarda suposi√ß√£o inicial de separabilidade linear dos dados, sobre a qual o algoritmo Perceptron foi constru√≠do, sua aplica√ß√£o sobre dados n√£o linearmente separ√°veis √© poss√≠vel na condi√ß√£o de toler√¢ncia √† ocorr√™ncia de erros de classifica√ß√£o. Para esse fim, √© adotada uma estrat√©gia de relaxamento at√© que a converg√™ncia seja atingida.

As seguintes etapas abrangem: este processo:

Inicialize os contadores $q$, $q^{*}$, $k‚Üê0$
Inicialize $\textbf{w}^{(0)}$ e $\textbf{w}^{*}$
Enquanto n√£o convergir (erro nulo ou m√°ximo de itera√ß√µes), repita:

<div align="center">
  
Aplique $g(\textbf{x})=\textbf{w}^{(k)^{T}}$ sobre cada $\textbf{x}_{i}$ em $D$

  $q ‚Üê$ n√∫mero de acertos utilizando $\textbf{w}^{(k)}$

  Se $q > q^{*}$ ent√£o:

  $q^{*} ‚Üê q$

  $\textbf{w}^{*} ‚Üê \textbf{w}^{(k)}$

k $‚Üê$ k+1

$\textbf{w}^{(k+1)} - Œ∑_{k}Œ¥(\textbf{x}_{i};\textbf{w}_{(k)})$

Atualize k $‚Üê$ k + 1 </div>

O desempenho desta estrat√©gia √© verificado atrav√©s da aplica√ß√£o mostrada na Figura 2.6. Para dados linearmente separ√°veis, tal estrat√©gia √© reduzida √† vers√£o original do algoritmo. Por outro lado, na condi√ß√£o de dados n√£o linearmente separ√°veis, o algoritmo termina ao atingir um n√∫mero m√°ximo de itera√ß√µes, e a solu√ß√£o √© dada pela configura√ß√£o de $\textbf{w}$ que proporciona maior acur√°cia entre todos os testes efetuados.

<div align="center">

![figura6](images/figura26.png "figura2.6") <legend> Figura 2.6 - Aplica√ß√£o do algoritmo Perceptron, baseado na estrat√©gia de relaxamento, sobre dados (a) linearmente separ√°veis e (b) n√£o linearmente separ√°veis..</legend> </div>
