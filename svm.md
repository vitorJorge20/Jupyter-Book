<style>
    legend {
        font-size: 16px;
    }
    main {
        text-align: justify;
    }
</style>

# 2.4 M√°quina de vetores de suporte (SVM) - vers√£o linear

O algoritmo de M√°quina de Vetores de Suporte (SVM - $\textit{Support Vector Machine}$), √© um algoritmo de aprendizado supervisionado que pode ser usado para classifica√ß√£o ou regress√£o. Ele √© um dos algoritmos mais populares de aprendizado de m√°quina e √© usado em uma ampla variedade de aplica√ß√µes, incluindo reconhecimento de imagens, classifica√ß√£o de texto e processamento de sinais.

O SVM funciona encontrando um hiperplano no espa√ßo de caracter√≠sticas que separa os dados de duas classes diferentes. O hiperplano √© escolhido de forma a maximizar a margem entre os dados das duas classes. A margem √© a dist√¢ncia entre o hiperplano e os pontos de dados mais pr√≥ximos de cada classe.

Para encontrar o hiperplano com a margem m√°xima, o algoritmo SVM usa um processo de otimiza√ß√£o que minimiza uma fun√ß√£o de custo. A fun√ß√£o de custo √© escolhida de forma a garantir que o hiperplano esteja suficientemente separado dos dados de cada classe.

O SVM pode ser usado para classificar dados linearmente separ√°veis ou n√£o linearmente separ√°veis. No caso de dados linearmente separ√°veis, o hiperplano pode ser encontrado diretamente usando um m√©todo de programa√ß√£o linear. No caso de dados n√£o linearmente separ√°veis, o algoritmo SVM usa uma fun√ß√£o kernel para transformar os dados para um espa√ßo de caracter√≠sticas de maior dimens√£o, onde eles podem ser separados linearmente. A seguir, ser√£o apresentadas as duas estrat√©gias.

## 2.4.1 Formaliza√ß√£o baseada em padr√µes linearmente separ√°veis

O m√©todo SVM consiste em definir uma fun√ß√£o discriminante linear $g$ capaz de classificar um conjunto de padr√µes linearmente separ√°vel $D=\{(\textbf{x}_{i},y_{i}):i=1,...,m\}$ entre as classes $œâ_{1}$, quando $y_{i}=+1$, e $w_{2}$, se $y_{i}=-1$. Al√©m disso, a fun√ß√µes discriminante em quest√£o busca pela maior margem de separa√ß√£o entre os exemplos deste par de classes.

A fun√ß√£o discriminante em quest√£o, segundo o m√©todo SVM, corresponde ao hiperplano que distingue os exemplos de $D$. Sua express√£o √© equivalente √† forma discutida na Equa√ß√£o 2.1 e exemplificada pelas discuss√µes da Figura 2.1. Um simples detalhe de nota√ß√£o refere-se √† troca de nota√ß√£o do termo $œâ_{0}$ por $b$, cujo significado n√£o sofre altera√ß√£o.

A Figura 2.7 apresenta dois exemplos de hiperplanos, $g_{1}$ e $g_{2}$. Embora ambos os hiperplanos sejam capazes de separar adequadamente os conjuntos de padr√µes $‚ãÜ$ e $\circ$, o hiperplano $g_{1}$ apresenta maior margem de separa√ß√£o. Assim, tal hiperplano possui maiores condi√ß√µes de manter a separabilidade entre as classes caso novos padr√µes sejam considerados, isto √©, sua capacidade de generaliza√ß√£o √© maior em compara√ß√£o a $g_{2}$.

<div align="center">

![figura7](images/figura27.png "figura2.7") <legend>Figura 2.7 - Padr√µes linearmente separ√°veis e hiperplanos capazes de efetuar a separa√ß√£o entre as classes. O hiperplano g2 em b) proporciona maior margem de separa√ß√£o.</legend> </div>

A determina√ß√£o do hiperplano associado a maior margem de separa√ß√£o √© baseada na simples rela√ß√£o de dist√¢ncia entre ponto e reta. A dist√¢ncia entre um padr√£o (ponto) $\textbf{x}_{i}$ qualquer e o hiperplano de separa√ß√£o (reta) $g(\textbf{x})=0$ √© calculada por $\frac{|g(\textbf{x}_{i})|}{||\textbf{w}||}$. Reescalonando $\textbf{w}$ de forma que dois padr√µes $\textbf{x}_{u} \in œâ_{1}$ e $\textbf{x}_{v} \in œâ_{2}$, ambos contidos em $D$ e caracterizados como os mais pr√≥ximos de $g(\textbf{x})=0$, estejam distantes √† uma unidade deste hiperplano, s√£o obtidas as seguintes rela√ß√µes:

<div align="center">

(i) a margem de separa√ß√£o tem largura $\frac{1}{||\textbf{w}||}+\frac{1}{||\textbf{w}||}=\frac{2}{||\textbf{w}||}$

(ii) dado $\textbf{x}_{u} \in œâ_{1}$, ent√£o $g(\textbf{w}_{u})= \textbf{w}^{T}\textbf{x}_{u}+b \geq +1$

(iiI) dado $\textbf{x}_{v} \in œâ_{2}$, ent√£o $g(\textbf{w}_{v})= \textbf{w}^{T}\textbf{x}_{v}+b \leq -1$ </div>

Baseado nessas rela√ß√µes, √© modelado o seguinte problema de otimiza√ß√£o, cuja solu√ß√£o leva aos par√£metros $\textbf{w}$ e $b$, que proporcionam o hiperplano com m√°xima margem de separa√ß√£o:

<div align="center">

$\begin{equation}
\min_{\textbf{w},b} \frac{1}{2}\textbf{w}^{T}\textbf{w} \tag{2.11}
\end{equation}$ 

$\begin{equation}
sujeito \ a: y_{i}(\textbf{w}^{T}\textbf{x}_{i}+b)\geq 1, i=1,...,m
\end{equation}$  </div>

A rela√ß√£o (i) √© utilizada na defini√ß√£o da fun√ß√£o objetivo de (2.11), uma vez que a minimiza√ß√£o de $||w||$ (reescrita na forma equivalente $\textbf{w}^{T}\textbf{w}$) torna m√°xima a margem de largura $\frac{2}{||\textbf{w}||}$. As rela√ß√µes (ii) e (iii), associados aos respectivos inidicadores de classe $y_{i}$, constituem a restri√ß√£o do problema de otimiza√ß√£o. Tal restri√ß√£o obriga o sinal da fun√ß√£o discriminante aplicada em $\textbf{x}_{i}$, isto √©, $g(\textbf{x}_{i})= \textbf{w}^{T}\textbf{x}_{i}+b$, a ser concordante com o sinal do indicador $y_{i}$, resultando, assim em uma multiplica√ß√£o igual ou superior a um.

A Figura 2.8 ilustra as rela√ß√µes geom√©tricas que o hiperplano de margem m√°xima deve respeitar. Cabe ressaltar que os padr√µes localizados nos limites das margem de separa√ß√£o (i.e., $g(\textbf{x}=\pm 1)$) s√£o respons√°veis por determinar o hiperplano √≥timo. Esses padr√µes s√£o denominados $\textbf{vetores de suporte}$.

<div align="center">

![figura8](images/figura28.png "figura2.8") <legend>Figura 2.8 - Representa√ß√£o do hiperplano de separa√ß√£o, vetores de suporte (circulados) e margem de separa√ß√£o.</legend> </div>

O problema de otimiza√ß√£o que determina o hiperplano de margem m√°xima possui fun√ß√£o objetivo convexa com restri√ß√µes lineares, logo torna-se conveniente resolv√™-lo com uso do M√©todo dos $\textbf{Multiplicadores de Lagrange}$. Para isso, inicialmente deve ser constru√≠da a fun√ß√£o lagrangeana, que consiste na soma da fun√ß√£o objetivo do problema (3.11) com combina√ß√£o linear formada pelas restri√ß√µes deste problema multiplicadas por escalares n√£o negativos $Œª_{i}$, denominados multiplicadores de Lagrange. A seguinte express√£o corresponde √† forma primal da fun√ß√£o lagrangeana do problema (3.11):

<div align="center">

$\begin{equation}
L_{p}(\textbf{w},b,Œª) = \frac{1}{2}\textbf{w}^{T}\textbf{w} - \sum_{i=1}^{m} Œª[y_{i}(\textbf{w}^{T}\textbf{x}_{i}+b)-1] \tag{2.12}
\end{equation}$ </div>

A solu√ß√£o do problema (2.11) √© equivalente ao "ponto de sela" da fun√ß√£o lagrangeana (2.12), a qual deve ser minimizada com rela√ß√£o a $\textbf{w}$ e $b$ e m√°ximizada com rela√ß√£o a $Œª_{i}$. Diante de sua caracter√≠stica convexa, o valor m√≠nimo da fun√ß√£o (3.12) com rela√ß√£o a $\textbf{w}$ e $b$ equivale ao ponto em que sua derivada, com rela√ß√£o a estes par√¢metros, √© nula. Por conseguinte, surgem as rela√ß√µes:

<div align="center">

$\begin{equation}
\frac{‚àÇL_{p}(\textbf{w},b,Œª)}{‚àÇ\textbf{w}}=0 ‚áí \textbf{w} = \sum_{i=1}^{m} Œª_{i}y_{i}\textbf{x}_{i} \tag{2.13}
\end{equation}$

$\begin{equation}
\frac{‚àÇL_{p}(\textbf{w},b,Œª)}{‚àÇb}=0 ‚áí \textbf{w} = \sum_{i=1}^{m} Œª_{i}y_{i}=0\tag{2.14}
\end{equation}$ </div>

Substituindo as rela√ß√µes (3.13) e (3.14) na fun√ß√£o lagrangeana (3.12), obt√©m-se como resultado a forma dual da fun√ß√£o lagrangeana:

<div align="center">

$\begin{equation}
L_{D}(Œª) = \sum_{i=1}^{m}Œª-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}Œª_{i}Œª_{j}y_{i}y_{j}\textbf{x}_{i}^{T}\textbf{x}_{j} \tag{2.15}
\end{equation}$ </div>

Com a elimina√ßa√µ de $\textbf{w}$ e $b$ nesta nova representa√ß√£o, a solu√ß√£o do problema (3.11) torna-se equivalente a resolu√ß√£o do seguinte problema de otimiza√ß√£o:

<div align="center">

$\begin{equation}
\max_{\lambda}L_{D}(\lambda)
\end{equation}$

$\begin{equation}
sujeito \ a \left \{ \begin{matrix} \lambda_{i}\geq 0,i=1,...,m \\ \sum_{i=1}^{m} Œª_{i}y_{i} = 0\end{matrix} \right.\tag{2.16}
\end{equation}$ </div>

Os multiplicadores de Lagrange obtidos com a resolu√ß√£o do problema (2.16) permitem o c√°lculo dos par√¢metros que definem o hiperplano de margem m√°xima. O vetor $\textbf{w}$ √© definido atrav√©s da rela√ß√£o (2.13), j√° o escalar $b$ pode ser calculado com a substitui√ß√£o de $\textbf{w}$ em $g(\textbf{x}_{i})$, sendo $\textbf{x}_{i}$ um vetor suporte com $y_{i}=+1$, logo $\textbf{w}^{T}\textbf{x}_{i}+b=1$.

Al√©m de proporcionar maior simplicidade no c√°lculo dos par√¢metros √≥timos, a representa√ß√£o dual da fun√ß√£o lagrangeana possui como fator o produto interno entre os padr√µes (i.e., $\textbf{x}_{i}^{T}\textbf{x}_{j}$), o que possibilita o uso de fun√ß√µes $\textbf{kernel}$, as quais ser√£o discutidas nas se√ß√µes seguintes.

Em consequ√™ncia ao hiperplano de margem m√°xima obtido, √© definida a fun√ß√£o discriminante $g(\textbf{x})=\textbf{w}^{T}\textbf{x}+b$, que classifica um dado padr√£o $\textbf{x}$ de acordo com a seguinte regra:

<div align="center">

$\begin{equation}
g(\textbf{x})=\textbf{w}^{T}\textbf{x}+b \left \{ \begin{matrix} \geq 0 ‚áí \textbf{x} \in œâ_{1} \\ < 0 ‚áí \textbf{x} \in œâ_{2} \end{matrix} \right. \tag{2.17}
\end{equation}$ </div>

## 2.4.2 Formaliza√ß√£o baseada em padr√µes n√£o linearmente separ√°veis

O desenvolvimento apresentado na se√ß√£o anterior √© fundamentamentado na suposi√ß√£o de padr√µes linearmente separ√°veis. No entante, √© natural a exist√™ncia de problemas envolvendo padr√µes que n√£o s√£o linearmente separ√°veis. A fim de contornar este tipo de problema, s√£o inseridas vari√°veis de folga $\xi \geq 0$ que tornam sempre verdadeiras as seguintes rela√ß√µes:

<div align="center">

(i) dado $\textbf{x}_{i} \in ùúî_{1}$, ent√£o $g(\textbf{x}_{i})=\textbf{w}^{T}\textbf{x}+b\geq+1-\xi_{i}$

(ii) dado $\textbf{x}_{i} \in ùúî_{2}$, ent√£o $g(\textbf{x}_{i})=\textbf{w}^{T}\textbf{x}+b\geq+1-\xi_{i}$ </div>

Geometricamente, as vari√°veis de folga respresentam o deslocamento em que os padr√µes classificados erroneamente encontram-se do limite da margem de separa√ß√£o referente a sua respectiva classe, conforme ilustra a Figura 2.9.


<div align="center">

![figura9](images/figura29.png "figura2.9") <legend>Figura 2.9 -Representa√ß√£o do hiperplano de separa√ß√£o para padr√µes n√£o linearmente separ√°veis.</legend> </div>

Com intuito de incorporar um custo adicional devido √† n√£o separabilidade dos padr√µes, √© introduzido o termo $C\sum_{i=1}^{m} \xi_{i} $ na fun√ß√£o objetivo do problema (2.10). Esse termo √© respons√°vel por contabilizar e penalizar a ocorr√™ncia de classifica√ß√µes incorretas segundo o hiperplano de separa√ß√£o definido. Neste contexto, $C>0$ atua como  um par√¢metro regulador. Assim, √© proporcionada a seguinte reformula√ß√£o sobre o problema (2.10):

<div align="center">

$\begin{equation}
\min_{\textbf{w},b} \frac{1}{2}\textbf{w}^{T}\textbf{w} + C\sum_{i=1}^{m} \xi_{i}
\end{equation}$

$\begin{equation}
sujeito \ a: \left\{\begin{matrix} y_{i}(\textbf{w}^{T}\textbf{x}_{i}+b)\geq 1-\xi_{i}, i=1,...,m \\ \xi_{i} > ; \ i=1,...m \end{matrix}\right. \tag{2.18}
\end{equation}$

Por sua vez, a fun√ß√£o Lagrangeana na forma primal obtida a partir do problema (2.18) √© exepressa por:

$\begin{equation}
L_{p}(\textbf{w},b,Œª) = \frac{1}{2}\textbf{w}^{T}\textbf{w}+C\sum_{i=1}^{m} \xi_{i} - \sum_{i=1}^{m} Œª_{i}[y_{i}(\textbf{w}^{T}\textbf{x}_{i}+b)-1+\xi_{i}] - \sum_{i=1}^{m} ùúå_{i}\xi_{i} \tag{2.19}
\end{equation}.$ </div>

em que $ùúå$ √© multiplicador de Lagrange inserido para garantir a positividade das vari√°veis de folga $\xi$.

Seguindo os mesmos procedimentos apresentados na subse√ß√£o anterior para obten√ß√£o da forma dual da fun√ß√£o lagrangeana, as derivadas da fun√ß√£o (2.19) com rela√ß√£o a $\textbf{w}$, $b$ e $\xi$ devem ser nulas. O resultado dessas deriva√ß√µes em rela√ß√£o a $\textbf{w}$ e $b$ produz os mesmos resultados obtidos em (2.13) e (2.14). J√° a deriva√ß√£o com rela√ß√£o a $\xi$ fornece que:

<div align="center">

$\begin{equation}
\frac{\partial L_{P}(\textbf{w},b,Œª),ùúå}{\partial \xi_{i}}=0 \tag{2.20}
\end{equation}$ </div>

logo, sendo $Œª$ e ùúå escalares reais estritamente positivos, conclu√≠mos que $0\leq Œª_{i} \leq C$.

Ap√≥s as devidas substitui√ß√µes, a forma dual da fun√ß√£o lagrangeana para o caso n√£o separ√°vel torna-se id√™ntica √† (2.15). Com isso, o problema de otimiza√ß√£o formulado para o tratamento de padr√µes n√£o linearmente separ√°veis difere do problema (2.16) apenas com rela√ß√£o √†s restri√ß√µes, ou seja:

<div align="center">

$\begin{equation}
\max_{Œª}\sum_{i=1}^{m} Œª_{i} - \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m} Œª_{i}Œª_{j}y_{i}y_{j}\textbf{x}_{i}^{T}\textbf{x}_{j}
\end{equation}$

$\begin{equation}
sujeito \ a:\left\{\begin{matrix} 0 \leq \lambda_{i} \leq C, i=1,...,m \\ \sum_{i=1}^{m} \lambda_{i}y_{i}=0 \end{matrix}\right. \tag{2.21}
\end{equation}$ </div>

Com a otimiza√ß√£o do problema (3.21), a obten√ß√£o do par√¢metro $\textbf{w}$ decorre da mesma forma realizada para o caso linearmente separ√°vel. J√° o c√°lculo de $b$ √© alcan√ßado a partir da primeira restri√ß√£o do problema (3.18), adotando qualquer vetor suporte cuja vari√°vel de folga seja nula.

Com o intuito de visualizar o comportamento dos hiperplanos de separa√ß√£o determinados atrav√©s do m√©todo SVM, a Figura 2.11 ilustra a aplica√ß√£o deste m√©todo sobre dados linearmente separ√°veis e n√£o linearmente separ√°veis. A respeito do caso envolvendo dados linearmente separ√°veis, √© poss√≠vel observar que a superf√≠cie de decis√£o, quando comparada aos m√©todos discutidos nas se√ß√µes anteriores, encontra-se mais afastada dos exemplos de treinamento que ocupam a regi√£o de transi√ß√£o. Sua aplica√ß√£o sobre dados n√£o linearmente separ√°veis proporciona uma superf√≠cie de decis√£o similar √†s obtidas pelos m√©todos abordados anteriormente.

<div align="center">

![figura10](images/figura210.png "figura2.10") <legend>Figura 2.10 - Hiperplanos de separa√ß√£o obtidos pelo SVM diante de dados linearmente separ√°veis (esquerda) e n√£o linearmente separ√°veis (direita).</legend> </div>

Por outro lado, diferentemente dos m√©todos discutidos nas se√ß√µes anteriores, em especial devido tanto √† dificuldade quanto √† estrutura√ß√£o e resolu√ß√£o do problema dado pela Equa√ß√£o 2.21, n√£o ser√° realizada a implementa√ß√£o do m√©todo SVM. Neste caso, a implementa√ß√£o disponibilizada pela biblioteca Scikit-Learn √© uma alternativa. Al√©m de conter fun√ß√µes relacionadas ao m√©todo SVM, esta biblioteca inclui ainda funcionalidades que ser√£o abordadas nos t√≥picos que seguem.

O C√≥digo 2.4 exibe um conjunto de comandos que contemplam a instancia√ß√£o do objeto que representa o classificador, seguido pleo seu treinamento e aplica√ß√£o na predi√ß√£o do indicador de classe de um dado padr√£o. Na linha 16, durante a instancia√ß√£o do objeto $g$, s√£o observados os par√¢metros $C$ e $\mathbb{kernel}$, os quais se referem √† penalidade (C) e ao tipo de fun√ß√£o kernel adotada, neste caso, linear, implicando, assim, que o produto interno $\textbf{x}_{i}^{T}\textbf{x}_{j}$, presente na Equa√ß√£o 2.21 n√£o seja substitu√≠do por uma outra forma alternativa.

```
#Importa√ß√£o da "Scikit-Learn" das fun√ß√µes relacionadas ao SVM
from sklearn import svm

#Obten√ß√£o de exemplos de treinamento (Dataset IRIS) (3 classes)
from sklearn import svm, datasets
from sklearn.model_selection import train_test_split #Fun√ß√£o que divide o conjunto de dados em treinamento e valida√ß√£o
iris = datasets.load_iris()

X = iris.data[:, :2]  # Selecionando apenas as duas primeiras caracter√≠sticas
y = iris.target

#Divis√£o dos conjunto de treino e valida√ß√£o
X_train,X_test,Y_train,Y_test = train_test_split(X,y,test_size = 0.33,random_state=42)

#Instancia√ß√£o do classificador "g"
g = svm.SVC(C=100, kernel='linear')

#Treinamento do modelo
g.fit(X_train,Y_train)

#Predi√ß√£o do indicador de classe
pred = g.predict(X_test)

#Acur√°cia
import numpy as np
acc = round(np.mean(pred == Y_test)*100,2)
print(acc)
```

A partir desse primeiro emprego da Scikit-Learn, vale destacar dois elementos que ser√£o obsercados com muita frequ√™ncia nos pr√≥ximos usos desta biblioteca: a instancia√ß√£o do m√©todo e a utiliza√ß√£o das fun√ß√µes $\mathbb{fit}$ e $\mathbb{predict}$. Na instancia√ß√£, √© criado um objeto relativo ao m√©todo em quest√£o. Por outro lado, ao passo que $\mathbb{fit}$ efetua o treinamento do m√©todo, $\mathbb{predict}$ atua na rotula√ß√£o de padr√µes, ou seja, duas etapas elementares em um processo de classifica√ß√£o. A maioria dos m√©todos implementados na biblioteca Scikit-Learn est√£o munidos das fun√ß√µes $\mathbb{fit}$ e $\mathbb{predict}$. Para uma verifica√ß√£o mais ampla a respeito da estrutura e das fun√ß√µes presentes nesta biblioteca, √© sugerida a leitura do "guia do usu√°rio".

## 2.4.3 Otimiza√ß√£o quadr√°tica, par√¢metros e o custo computacional

O m√©todo SVM apresenta certas vantagens em rela√ß√£o a outros m√©todos, a citar, aqueles baseados na ideia do gradiente descenedente ou no ajuste de distribui√ß√µes de probabilidade. Nestes exemplos, o aprendizado √© sens√≠vel aos par√¢metros de inicializa√ß√£o e pode conduzir a resultados sub√≥timos (m√≠nimos locais). Ao contr√°rio, o processo de aprendizado do m√©todo SVM √© realizado pela otimiza√ß√£o de problemas quadr√°ticos convexos, o que implica em solu√ß√µes √∫nicas e definidas explicas explicitamente.

Uma fragilidade decorrente do problema de otimiza√ß√£o associado ao SVM refere-se ao n√∫mero de parcelas da fun√ß√£o objetivo, que cresce quadraticamente em rela√ß√£o ao n√∫mero de padr√µes de treinamento. Esta caracter√≠stica torna cr√≠tico seu aprendizado diante de grandes conjuntos de dados. Tal fato tem motivado, ao longo do anos, o desenvolvimento e aprimoramento de algoritmos capazes de otimizar o processo de aprendizado. Exemplos de algoritmos destinados a esse prop√≥sito s√£o $\textit{Sequential Minimal Optimization}$ (SMO), $SVM^{Light}$, $\textit{Chuncking}$ e $LibSVM$.

Outro fator que atua diretamente no custo computacional s√£o os par√¢metros adotados, sejam eles referentes √† fun√ß√£o $\mathbb{kernel}$ ou √† penalidade de erro (C). A ado√ß√£o de par√¢metros que minimizam o custo computacional pode prejudicar a capacidade de aprendizado e generaliza√ß√£o do m√©todo, proporcionando, assim, resultado insatisfat√≥rios.

Dessa forma, podemos concluir que, embora o m√©todo de otimiza√ß√£o seja um coadjuvante importante no custo computacional, a quantidade de informa√ß√£o utilizada no treinamento e os par√¢metros adotados tornam-se decisivos no processo de aprendizado.

